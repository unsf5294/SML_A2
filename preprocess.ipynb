{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f97be85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers sentence_transformers pandas nltk numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3922e3e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总共保存 10000 条数据到 data/data.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 读取三个 CSV 文件\n",
    "train_df = pd.read_csv('cross_domains_cross_models/train.csv')\n",
    "test_df = pd.read_csv('cross_domains_cross_models/test.csv')\n",
    "valid_df = pd.read_csv('cross_domains_cross_models/valid.csv')\n",
    "\n",
    "# 按行合并三个 DataFrame\n",
    "combined_df = pd.concat([train_df, test_df, valid_df], ignore_index=True)\n",
    "\n",
    "subset_df = combined_df.sample(n=10000, random_state=42)\n",
    "\n",
    "# 保存为新的 CSV 文件\n",
    "subset_df.to_csv('data/data.csv', index=False)\n",
    "\n",
    "print(f\"{len(subset_df)} into data/data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5740342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Distribution:\n",
      "  Label 0: 281824 samples (65.13%)\n",
      "  Label 1: 150858 samples (34.87%)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "unique, counts = np.unique(combined_df[\"label\"].values, return_counts=True)\n",
    "label_dist = dict(zip(unique, counts))\n",
    "\n",
    "print(\"Label Distribution:\")\n",
    "for label, count in label_dist.items():\n",
    "    percent = count / counts.sum() * 100\n",
    "    print(f\"  Label {label}: {count} samples ({percent:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54561bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib \n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "deepfake_name_dct = {'OpenAI-GPT':['gpt-3.5-trubo','text-davinci-002', 'text-davinci-003'],\n",
    "            'Meta-LLaMA':['13B', '30B', '65B', '7B'],\n",
    "            'GLM-130B':['GLM130B'],\n",
    "            'Google-FLAN-T5':['flan_t5_base', 'flan_t5_large','flan_t5_small', 'flan_t5_xl', 'flan_t5_xxl'],\n",
    "            'Facebook-OPT':['opt_1.3b', 'opt_125m', 'opt_13b', 'opt_2.7b', 'opt_30b', 'opt_350m', 'opt_6.7b', 'opt_iml_30b','opt_iml_max_1.3b'],\n",
    "            'BigScience':['bloom_7b','t0_11b', 't0_3b'],\n",
    "            'EleutherAI':['gpt_j','gpt_neox'],\n",
    "            'human':['human']}\n",
    "deepfake_model_set ={'OpenAI-GPT':0,'Meta-LLaMA':1,'GLM-130B':2,'Google-FLAN-T5':3,\n",
    "            'Facebook-OPT':4,'BigScience':5,'EleutherAI':6,'human':7}\n",
    "\n",
    "def stable_long_hash(input_string):\n",
    "    hash_object = hashlib.sha256(input_string.encode())\n",
    "    hex_digest = hash_object.hexdigest()\n",
    "    int_hash = int(hex_digest, 16)\n",
    "    long_long_hash = (int_hash & ((1 << 63) - 1))\n",
    "    return long_long_hash\n",
    "\n",
    "def process_data(dataset):\n",
    "    data_list=[]\n",
    "    for i in range(len(dataset)):\n",
    "        text,label,src=dataset[i]['text'],str(dataset[i]['label']),dataset[i]['src']\n",
    "        data_list.append((text,label,src,stable_long_hash(text)))\n",
    "    return data_list\n",
    "\n",
    "def load_deepfake(data_file='data.csv'):\n",
    "    \"\"\"\n",
    "    读取合并后的 data.csv 并返回处理后的列表\n",
    "    \"\"\"\n",
    "    if not os.path.exists(data_file):\n",
    "        raise FileNotFoundError(f\"{data_file} 不存在，请先生成 data.csv\")\n",
    "    \n",
    "    df = pd.read_csv(data_file, usecols=[\"text\", \"label\", \"src\"])\n",
    "    \n",
    "    # 转换为字典列表\n",
    "    data_dict_list = []\n",
    "    for i in range(len(df)):\n",
    "        dct = {\n",
    "            'text': df.loc[i, 'text'],\n",
    "            'label': df.loc[i, 'label'],\n",
    "            'src': df.loc[i, 'src']\n",
    "        }\n",
    "        data_dict_list.append(dct)\n",
    "    \n",
    "    processed_data = process_data(data_dict_list)\n",
    "    \n",
    "    return processed_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ab1dde2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共读取 10000 条文本\n",
      "正在加载模型...\n",
      "正在生成 embedding...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1b3243aeaf6422aa058c03c4847b02f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding 已保存到 data/text_embeddings.npy\n",
      "embedding 形状: (10000, 384)\n"
     ]
    }
   ],
   "source": [
    "# embedding_data.py\n",
    "# Requires transformers>=4.51.0\n",
    "# Requires sentence-transformers>=2.7.0\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "\n",
    "# -------------------------\n",
    "# 配置\n",
    "# -------------------------\n",
    "DATA_FILE = \"data/data.csv\"           # 输入数据\n",
    "EMBEDDING_FILE = \"data/text_embeddings.npy\"  # 保存 embedding 的文件\n",
    "MODEL_NAME = \"sentence-transformers/msmarco-MiniLM-L6-v3\"\n",
    "\n",
    "# -------------------------\n",
    "# 读取数据\n",
    "# -------------------------\n",
    "if not os.path.exists(DATA_FILE):\n",
    "    raise FileNotFoundError(f\"{DATA_FILE} 不存在，请先生成。\")\n",
    "\n",
    "df = pd.read_csv(DATA_FILE, usecols=[\"text\"])\n",
    "texts = df[\"text\"].tolist()\n",
    "print(f\"共读取 {len(texts)} 条文本\")\n",
    "\n",
    "# -------------------------\n",
    "# 加载模型\n",
    "# -------------------------\n",
    "print(\"正在加载模型...\")\n",
    "model = SentenceTransformer(MODEL_NAME)\n",
    "\n",
    "# -------------------------\n",
    "# 生成 embedding\n",
    "# -------------------------\n",
    "print(\"正在生成 embedding...\")\n",
    "embeddings = model.encode(texts, batch_size=32, show_progress_bar=True)\n",
    "\n",
    "# -------------------------\n",
    "# 保存 embedding\n",
    "# -------------------------\n",
    "np.save(EMBEDDING_FILE, embeddings)\n",
    "print(f\"embedding 已保存到 {EMBEDDING_FILE}\")\n",
    "print(f\"embedding 形状: {embeddings.shape}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
