{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d8a8a3c",
   "metadata": {},
   "source": [
    "## Load the dataset and embedding with sentence BERT and Tf-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3922e3e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 into data/data.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# read MEGA dataset\n",
    "train_df = pd.read_csv('cross_domains_cross_models/train.csv')\n",
    "test_df = pd.read_csv('cross_domains_cross_models/test.csv')\n",
    "valid_df = pd.read_csv('cross_domains_cross_models/valid.csv')\n",
    "\n",
    "# combine\n",
    "combined_df = pd.concat([train_df, test_df, valid_df], ignore_index=True)\n",
    "\n",
    "subset_df = combined_df.sample(n=10000, random_state=42)\n",
    "encoder_train = combined_df.drop(subset_df.index).sample(n=10000, random_state=43)\n",
    "\n",
    "# save\n",
    "subset_df.to_csv('data/data.csv', index=False)\n",
    "encoder_train.to_csv('data/encoder_data.csv', index=False)\n",
    "print(f\"{len(subset_df)} into data/data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5740342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Distribution:\n",
      "  Label 0: 281824 samples (65.13%)\n",
      "  Label 1: 150858 samples (34.87%)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "unique, counts = np.unique(combined_df[\"label\"].values, return_counts=True)\n",
    "label_dist = dict(zip(unique, counts))\n",
    "\n",
    "print(\"Label Distribution:\")\n",
    "for label, count in label_dist.items():\n",
    "    percent = count / counts.sum() * 100\n",
    "    print(f\"  Label {label}: {count} samples ({percent:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54561bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib \n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "deepfake_name_dct = {'OpenAI-GPT':['gpt-3.5-trubo','text-davinci-002', 'text-davinci-003'],\n",
    "            'Meta-LLaMA':['13B', '30B', '65B', '7B'],\n",
    "            'GLM-130B':['GLM130B'],\n",
    "            'Google-FLAN-T5':['flan_t5_base', 'flan_t5_large','flan_t5_small', 'flan_t5_xl', 'flan_t5_xxl'],\n",
    "            'Facebook-OPT':['opt_1.3b', 'opt_125m', 'opt_13b', 'opt_2.7b', 'opt_30b', 'opt_350m', 'opt_6.7b', 'opt_iml_30b','opt_iml_max_1.3b'],\n",
    "            'BigScience':['bloom_7b','t0_11b', 't0_3b'],\n",
    "            'EleutherAI':['gpt_j','gpt_neox'],\n",
    "            'human':['human']}\n",
    "deepfake_model_set ={'OpenAI-GPT':0,'Meta-LLaMA':1,'GLM-130B':2,'Google-FLAN-T5':3,\n",
    "            'Facebook-OPT':4,'BigScience':5,'EleutherAI':6,'human':7}\n",
    "\n",
    "def stable_long_hash(input_string):\n",
    "    hash_object = hashlib.sha256(input_string.encode())\n",
    "    hex_digest = hash_object.hexdigest()\n",
    "    int_hash = int(hex_digest, 16)\n",
    "    long_long_hash = (int_hash & ((1 << 63) - 1))\n",
    "    return long_long_hash\n",
    "\n",
    "def process_data(dataset):\n",
    "    data_list=[]\n",
    "    for i in range(len(dataset)):\n",
    "        text,label,src=dataset[i]['text'],str(dataset[i]['label']),dataset[i]['src']\n",
    "        data_list.append((text,label,src,stable_long_hash(text)))\n",
    "    return data_list\n",
    "\n",
    "def load_deepfake(data_file='data.csv'):\n",
    "    if not os.path.exists(data_file):\n",
    "        raise FileNotFoundError(f\"{data_file} not exist\")\n",
    "    \n",
    "    df = pd.read_csv(data_file, usecols=[\"text\", \"label\", \"src\"])\n",
    "    \n",
    "    data_dict_list = []\n",
    "    for i in range(len(df)):\n",
    "        dct = {\n",
    "            'text': df.loc[i, 'text'],\n",
    "            'label': df.loc[i, 'label'],\n",
    "            'src': df.loc[i, 'src']\n",
    "        }\n",
    "        data_dict_list.append(dct)\n",
    "    \n",
    "    processed_data = process_data(data_dict_list)\n",
    "    \n",
    "    return processed_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab1dde2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共读取 10000 条文本\n",
      "正在加载模型...\n",
      "正在生成 embedding...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1b3243aeaf6422aa058c03c4847b02f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding 已保存到 data/text_embeddings.npy\n",
      "embedding 形状: (10000, 384)\n"
     ]
    }
   ],
   "source": [
    "# embedding_data.py\n",
    "# Requires transformers>=4.51.0\n",
    "# Requires sentence-transformers>=2.7.0\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "\n",
    "# -------------------------\n",
    "# Configuration\n",
    "# -------------------------\n",
    "DATA_FILE = \"data/data.csv\"                 # Input data file\n",
    "EMBEDDING_FILE = \"data/text_embeddings.npy\"  # File to save embeddings\n",
    "MODEL_NAME = \"sentence-transformers/msmarco-MiniLM-L6-v3\"\n",
    "\n",
    "# -------------------------\n",
    "# Read data\n",
    "# -------------------------\n",
    "if not os.path.exists(DATA_FILE):\n",
    "    raise FileNotFoundError(f\"{DATA_FILE} does not exist. Please generate it first.\")\n",
    "\n",
    "df = pd.read_csv(DATA_FILE, usecols=[\"text\"])\n",
    "texts = df[\"text\"].tolist()\n",
    "print(f\"Loaded {len(texts)} text samples\")\n",
    "\n",
    "# -------------------------\n",
    "# Load model\n",
    "# -------------------------\n",
    "print(\"Loading model...\")\n",
    "model = SentenceTransformer(MODEL_NAME)\n",
    "\n",
    "# -------------------------\n",
    "# Generate embeddings\n",
    "# -------------------------\n",
    "print(\"Generating embeddings...\")\n",
    "embeddings = model.encode(texts, batch_size=32, show_progress_bar=True)\n",
    "\n",
    "# -------------------------\n",
    "# Save embeddings\n",
    "# -------------------------\n",
    "np.save(EMBEDDING_FILE, embeddings)\n",
    "print(f\"Embeddings saved to {EMBEDDING_FILE}\")\n",
    "print(f\"Embedding shape: {embeddings.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752e0dc8",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779c2ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF shape: (10000, 56111)\n",
      "降维后 shape: (10000, 384)\n",
      "保留方差比例: 0.2758343709159965\n",
      "Embedding 已保存到 data/text_embedding_tfidf.npy\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import os\n",
    "\n",
    "DATA_FILE = \"data/data.csv\"\n",
    "OUTPUT_FILE = \"data/text_embedding_tfidfs.npy\"\n",
    "\n",
    "# Read text data\n",
    "df = pd.read_csv(DATA_FILE, usecols=[\"text\"])\n",
    "texts = df[\"text\"].tolist()\n",
    "\n",
    "# TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf = vectorizer.fit_transform(texts)  # Returns a sparse matrix\n",
    "print(\"TF-IDF shape:\", tfidf.shape)\n",
    "\n",
    "# Dimensionality reduction using TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=384, random_state=42)\n",
    "tfidf_reduced = svd.fit_transform(tfidf)  # Outputs a dense matrix\n",
    "print(\"Reduced shape:\", tfidf_reduced.shape)\n",
    "print(\"Explained variance ratio:\", svd.explained_variance_ratio_.sum())\n",
    "\n",
    "# Save embeddings\n",
    "np.save(OUTPUT_FILE, tfidf_reduced)\n",
    "print(f\"Embeddings saved to {OUTPUT_FILE}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
